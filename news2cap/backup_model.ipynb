{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# By now, we should know that pytorch has a functional implementation (as opposed to class version)\n",
    "# of many common layers, which is especially useful for layers that do not have any parameters.\n",
    "# e.g. relu, sigmoid, softmax, etc.\n",
    "from utils import *\n",
    "from util_lstm import LSTM,entangledLSTMCell\n",
    "from data_preprocessing import CocoCaptions,customBatchBuilder\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "class ImageTextGeneratorModel(nn.Module):\n",
    "    # The model has three layers: \n",
    "    #    1. An Embedding layer that turns a sequence of word ids into \n",
    "    #       a sequence of vectors of fixed size: embeddingSize.\n",
    "    #    2. An RNN layer that turns the sequence of embedding vectors into \n",
    "    #       a sequence of hiddenStates.\n",
    "    #    3. A classification layer that turns a sequence of hidden states into a \n",
    "    #       sequence of softmax outputs.\n",
    "    def __init__(self, vocabularySize,imageFeatureSize,entangled_size):\n",
    "        super(ImageTextGeneratorModel, self).__init__()\n",
    "        # See documentation for nn.Embedding here:\n",
    "        # http://pytorch.org/docs/master/nn.html#torch.nn.Embedding\n",
    "        self.embedder = nn.Embedding(vocabularySize, 2048)\n",
    "\n",
    "        self.Image2Embedding = nn.Linear(imageFeatureSize, 2048)\n",
    "        self.rnn = LSTM(entangledLSTMCell, 2048, 1024 , factor_size=1024, entangled_size=entangled_size, batch_first = False)\n",
    "        self.classifier = nn.Linear(1024, vocabularySize)\n",
    "        self.vocabularySize = vocabularySize\n",
    "        #self.resnet = models.vgg16(pretrained=True)\n",
    "        #self.resnet = nn.Sequential(*list(self.resnet.children())[:-1])\n",
    "\n",
    "\n",
    "    # The forward pass makes the sequences go through the three layers defined above.\n",
    "    def forward(self, vggFeatures, tagFeatures, paddedSeqs, initialHiddenState=None, initialCellState=None):\n",
    "        \n",
    "        batchSequenceLength = paddedSeqs.size(0)  # 0-dim is sequence-length-dim.\n",
    "        batchSize = paddedSeqs.size(1)  # 1-dim is batch dimension.\n",
    "        \n",
    "        # Transform word ids into an embedding vector.\n",
    "        embeddingVectors = self.embedder(paddedSeqs)\n",
    "        #print Images\n",
    "        #print('vgg',vggFeatures)\n",
    "\n",
    "        ImageEmbeddings=self.Image2Embedding(vggFeatures)\n",
    "        #print('emb',ImageEmbeddings)\n",
    "        ImageEmbeddings = ImageEmbeddings.view(1,64,2048)\n",
    "        # Pass the sequence of word embeddings to the RNN.\n",
    "        embeddingVectors=torch.cat((ImageEmbeddings,embeddingVectors[1:]), 0)\n",
    "        if initialHiddenState!=None:\n",
    "            rnnOutput, (finalHiddenState, finalCellState) = self.rnn(embeddingVectors, hx=(initialHiddenState, initialCellState),entangler=tagFeatures)\n",
    "        else:\n",
    "            rnnOutput, (finalHiddenState, finalCellState) = self.rnn(embeddingVectors, entangler=tagFeatures)\n",
    "        \n",
    "        # Collapse the batch and sequence-length dimensions in order to use nn.Linear.\n",
    "        flatSeqOutput = rnnOutput.view(-1, 1024)\n",
    "        predictions = self.classifier(flatSeqOutput)\n",
    "        \n",
    "        # Expand back the batch and sequence-length dimensions and return. \n",
    "        return predictions.view(batchSequenceLength, batchSize, self.vocabularySize), \\\n",
    "               (finalHiddenState, finalCellState)\n",
    "        \n",
    "'''\n",
    "# Let's test the model on some input batch.\n",
    "f_aritcle=open('data/article_features/IND_dict.pickle',\"rb\")\n",
    "tag_features=pickle.load(f_aritcle)\n",
    "#print(tag_features)\n",
    "\n",
    "f_image=open('data/image_features/vggfeatures-IND.pickle',\"rb\")\n",
    "vgg_features=pickle.load(f_image)\n",
    "print(len(vgg_features['6bd6ca984a47ea8f9a74e87e465cc50df155e77f']))\n",
    "\n",
    "# Let's test the data class.\n",
    "trainData = CocoCaptions(['data/captions/IND-JSON/IND_Partial_0.jsonld','data/captions/IND-JSON/IND_Partial_1.jsonld','data/captions/IND-JSON/IND_Partial_2.jsonld'],tag_features=tag_features,img_features=vgg_features)\n",
    "print('Number of training examples: ', len(trainData))\n",
    "\n",
    "\n",
    "# It would be a mistake to build a vocabulary using the validation set so we reuse.\n",
    "valData = CocoCaptions(['data/captions/IND-JSON/IND_Partial_3.jsonld'],tag_features=tag_features,img_features=vgg_features, vocabulary = trainData.vocabulary)\n",
    "print('Number of validation examples: ', len(valData))\n",
    "\n",
    "\n",
    "# Data loaders in pytorch can use a custom batch builder, which we are using here.\n",
    "trainLoader = data.DataLoader(trainData, batch_size = 64, \n",
    "                              shuffle = True, num_workers = 0,\n",
    "                              collate_fn = customBatchBuilder)\n",
    "valLoader = data.DataLoader(valData, batch_size = 64, \n",
    "                            shuffle = False, num_workers = 0,\n",
    "                            collate_fn = customBatchBuilder)\n",
    "\n",
    "# Now let's try using the data loader.\n",
    "index, (imgIds, Tags, Imgs, paddedSeqs, seqLengths) = next(enumerate(trainLoader))\n",
    "\n",
    "\n",
    "\n",
    "vocabularySize = len(trainData.vocabulary['word2id'])\n",
    "model = ImageTextGeneratorModel(vocabularySize,4096,1024)\n",
    "#print model\n",
    "print(\"start\")\n",
    "model.eval()\n",
    "# Create the initial hidden state for the RNN.\n",
    "index, (imgIds, Tags, Imgs, paddedSeqs, seqLengths) = next(enumerate(trainLoader))\n",
    "print(len(Tags))\n",
    "Imgs=Variable(torch.from_numpy(np.array(Imgs)).float())\n",
    "Tags=Variable(torch.from_numpy(np.array(Tags)).float())\n",
    "\n",
    "print('pad',paddedSeqs.size(1))\n",
    "initialHiddenState=Variable(torch.Tensor(paddedSeqs.size(1), 1024).zero_())\n",
    "initialCellState=Variable(torch.Tensor(paddedSeqs.size(1), 1024).zero_())\n",
    "predictions, _ = model(Imgs, Tags,torch.autograd.Variable(paddedSeqs),initialHiddenState,initialCellState)\n",
    "\n",
    "print('outputs_size', predictions.size()) # 10 output softmax predictions over our vocabularySize outputs.\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
