{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "USE_CUDA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SOS_token = 0\n",
    "# EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {}\n",
    "        self.n_words = 0\n",
    "    \n",
    "    def index_words(self, sentence):\n",
    "        for word in word_tokenize(sentence.lower()):\n",
    "            self.index_word(word)\n",
    "    \n",
    "    def index_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1  \n",
    "            \n",
    "    def filterRareWords(self, vocabularySize = 8000):\n",
    "        sorted_words = sorted(list(self.word2count.items()), \n",
    "                              key = lambda x: -x[1])\n",
    "        most_frequent_words = [w for (w, c) in sorted_words[:vocabularySize]]\n",
    "        self.word2index = {w: (index + 1) for (index, w) in enumerate(most_frequent_words)}\n",
    "        self.word2index[\"[EOS]\"] = 0\n",
    "        self.word2index[\"[SOS]\"] = len(self.word2index)\n",
    "        self.word2index[\"[UNK]\"] = len(self.word2index)\n",
    "        self.index2word = {index: w for (w, index) in self.word2index.items()}\n",
    "        self.vocabulary = {'word2id': self.word2index, 'id2word': self.index2word}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_string(s):\n",
    "    s = re.sub(r\"([.!?])\", r\"\\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readArticle(fileName):\n",
    "#     print(\"reading articles...\")\n",
    "    \n",
    "    f = open(fileName)\n",
    "    rawLine = f.readline()\n",
    "    rawLine = normalize_string(rawLine).strip()\n",
    "    \n",
    "    f.close()\n",
    "    return rawLine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readCaption(fileName):\n",
    "#     print(\"reading captions...\")\n",
    "    \n",
    "    f = open(fileName)\n",
    "    rawLine = f.readline()\n",
    "    rawLine = normalize_string(rawLine).strip()\n",
    "    \n",
    "    f.close()\n",
    "    return rawLine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readArticleCaptionPairs(articleDirName, captionDirName):\n",
    "    pairs = []\n",
    "    \n",
    "    for fileName in os.listdir(articleDirName):\n",
    "        if fileName.endswith(\".txt\"):\n",
    "            articleFileName = os.path.join(articleDirName, fileName)\n",
    "            captionFileName = os.path.join(captionDirName, fileName)\n",
    "            if not os.path.isfile(captionFileName):\n",
    "                continue\n",
    "            articleContent = readArticle(articleFileName)\n",
    "            captionContent = readCaption(captionFileName)\n",
    "            \n",
    "            articleCaptionPair = (articleContent, captionContent)\n",
    "            pairs.append(articleCaptionPair)\n",
    "            \n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepareData(articleCaptionLang, articleDirName, captionDirName):\n",
    "    \n",
    "    pairs = readArticleCaptionPairs(articleDirName, captionDirName)\n",
    "    \n",
    "    for pair in pairs:\n",
    "        articleCaptionLang.index_words(pair[0])\n",
    "        articleCaptionLang.index_words(pair[1])\n",
    "    \n",
    "#     articleCaptionLang.filterRareWords()\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualizeWordDistribution(articleCaptionLang):\n",
    "    sorted_words = sorted(list(articleCaptionLang.word2count.items()), \n",
    "                              key = lambda x: -x[1])\n",
    "    totalVocSize = len(sorted_words)\n",
    "    print(\"totalVocSize\\t\", totalVocSize)\n",
    "    xList = []\n",
    "    yList = []\n",
    "    \n",
    "    for wordIndex in range(totalVocSize):\n",
    "#         print(wordIndex)\n",
    "        xList.append(wordIndex)\n",
    "        yList.append(articleCaptionLang.word2count[sorted_words[wordIndex][0]])\n",
    "    \n",
    "    print(np.mean(yList))\n",
    "    print(yList[8000])\n",
    "    plt.plot(xList, yList)\n",
    "    plt.show()\n",
    "    \n",
    "#     for word in sorted_words:\n",
    "#         xList.append()\n",
    "#     most_frequent_words = [w for (w, c) in sorted_words]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "articleCaptionLang = Lang(\"articleCaption\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# visualizeWordDistribution(articleCaptionLang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "articleCaptionLang.filterRareWords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "articleDirName = \"./IND-articles\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "captionDirName = \"./IND-captions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pairs=prepareData(articleCaptionLang, articleDirName, captionDirName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Threats to the future of the Department for Business are nothing new. Back in Vince Cable proposed abolishing the old Department for Trade and Industry because it doesn t perform a function . Happily Mr Cable had cooled on the idea by the time he was made Secretary of State for Business Innovation and Skills BIS for the Coalition in . But the rumours are back in the wake of last week s Tory election triumph. Adrian Bailey the chair of the BIS Commons Select Committee in the last Parliament believes David Cameron s Government s plans for some bn of Whitehall cuts could eventually mean BIS led by Sajid Javid after yesterday s reshuffle will be abolished and its responsibilities distributed across Whitehall. The Taxpayers Alliance lobby group recently estimated that closing and parcelling out the functions of BIS along with Energy and Culture to other departments could save . bn over five years. The Tory backbencher Dominic Raab is one of those who have lobbied for the end of BIS in recent years Some departments don t merit separate bureaucracies with all their associated costs churning out red tape he argued in . It s possible to see the logic of the dismantlers. BIS performs a very diverse range of functions from banging the drum for British exports to sponsoring apprenticeships administering employment tribunals and funding missions to space. They are functions that don t always fit together naturally. Labour has also provided some ammunition for the anti BIS brigade. It is an indifferent champion of business particularly small business and relates poorly to many key industrial sectors concluded Labour s Andrew Adonis in a review of the department two years ago. It is also far too centralised on London and lacks outreach to and sufficient knowledge of the real economy particularly outside the capital. So would BIS be missed if its Victoria Street headquarters in the shadow of Westminster Abbey was wiped off the face of the earth? Most business lobby groups say that actually it would. It is extremely important to have a department that advocates specifically for the interests of business and business growth in Whitehall says Adam Marshall director of policy at the British Chambers of Commerce. It is also very important that the department makes sure others can understand why the business perspective needs to be taken seriously. For that reason alone having a business department has a certain importance. Calls for a break up of BIS are a little crude says Simon Walker of the Institute of Directors. There s always a case for reform but a lot of the public expenditure from BIS is in areas that are of great importance to IoD members. John Cridland director general of the CBI says BIS plays a vital role in supporting growth and job creation. The lobby groups credit BIS in recent years with enhancing UK innovation through its catapult centres which assist private companies to test inventions and collaborate with research scientists. Others say BIS has helped finance industry. The single most important area where BIS has been a force for good is championing access to finance for growing businesses argues Mr Marshall of the BCC citing the creation of the British Business Bank by Mr Cable. The City also has supportive things to say We value the work UK Trade and Investment part of BIS has done promoting our industry overseas in areas such as attracting foreign money into UK funds says Daniel Godfrey chief executive of the Investment Association whose members manage more than trn for clients across the world. BIS has also been integral in helping us promote a long term investment culture in the UK. The bits we work with aren t broke so I don t see a need to fix them. Amanda Blanc chief executive of AXA s UK Commercial business agrees. No government department is ever likely to garner universal acclaim and BIS is no exception but the SME small and medium size enterprise sector has grown over the past five years and the efforts of the department have certainly been a contributing factor she says. One of the areas where one encounters some grumbling is in the field of regulation. If you look to the last Parliament there was a significant amount of employment regulation from BIS says Mr Marshall. You want BIS to be a champion of reducing or containing regulation rather than adding to it. Others say the skills promotion focus should shift to vital areas such as engineering. But if abolition is a non starter what about those budget cuts? BIS has a bn budget and arms length bodies that cover everything from copyright law to weather forecasting. With the Tories committed to make bn of departmental cuts to hits their deficit reduction goals it looks very vulnerable. Ministers might well see BIS as a tempting target since the public are unlikely to march in the streets to protect funding for say the Groceries Code Adjudicator or the British Hallmarking Council. Mr Marshall stresses that it makes no sense to demand cuts on growth promoting departments such as BIS while ring fencing health and pensions. Simon Walker from the IoD is keeping a watching brief on the threat There are going to be areas that have a higher return than others such as research and higher education but we will wait and see what is proposed he says. If the Treasury axe does look as if it is about to swing brutally in the direction of BIS in the run up to the next spending review the department will not be without some vocal private sector defenders. From Deutsche to BIS Sajid Javid s progress What sort of Business Secretary will Sajid Javid be? The appointment of the year old former Deutsche Bank investment banker was warmly greeted by the CBI yesterday He will be a strong voice for the business community helping to make its voice heard loud and clear at the Cabinet table John Cridland said. Simon Walker of the Institute of Directors said Mr Javid s immigrant background he is the son of Pakistani bus driver who came to the UK will help him to do the job well. He understands not only the reality of operating in the private sector but also the motivations Mr Walker said. The UK s banking lobby is hoping that Mr Javid s background will incline him to be less combative with the financial sector than his predecessor Vince Cable. But what if the Treasury demands deep cuts to BIS? Hopes that Mr Javid will put up strong resistance might be misplaced. He is often described as a prot g of the Chancellor George Osborne.', 'Are knives being sharpened for the Business Department?')\n"
     ]
    }
   ],
   "source": [
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def indexes_from_sentence(lang, sentence):\n",
    "    ids = []\n",
    "    for w in word_tokenize(sentence.lower()):\n",
    "        if w not in lang.word2index.keys():\n",
    "            ids.append(lang.word2index['[UNK]'])\n",
    "        else:\n",
    "            ids.append(lang.word2index[w])\n",
    "            \n",
    "    ids.insert(0, lang.word2index['[SOS]'])\n",
    "    ids.append(lang.word2index['[EOS]'])\n",
    "\n",
    "    return ids\n",
    "\n",
    "def variable_from_sentence(lang, sentence):\n",
    "    indexes = indexes_from_sentence(lang, sentence)\n",
    "    var = Variable(torch.LongTensor(indexes).view(-1, 1))\n",
    "#     print('var =', var)\n",
    "    if USE_CUDA: var = var.cuda()\n",
    "    return var\n",
    "\n",
    "def variables_from_pair(articleCaptionLang, pair):\n",
    "    input_variable = variable_from_sentence(articleCaptionLang, pair[0])\n",
    "    target_variable = variable_from_sentence(articleCaptionLang, pair[1])\n",
    "    return (input_variable, target_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_pair = variables_from_pair(articleCaptionLang, random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "       \n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "        \n",
    "    def forward(self, word_inputs, hidden):\n",
    "        # Note: we run this all at once (over the whole input sequence)\n",
    "        seq_len = len(word_inputs)\n",
    "        embedded = self.embedding(word_inputs).view(seq_len, 1, -1)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        hidden = Variable(torch.zeros(self.n_layers, 1, self.hidden_size))\n",
    "        if USE_CUDA: hidden = hidden.cuda()\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size, max_length=MAX_LENGTH):\n",
    "        super(Attn, self).__init__()\n",
    "        \n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        if self.method == 'general':\n",
    "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.other = nn.Parameter(torch.FloatTensor(1, hidden_size))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        seq_len = len(encoder_outputs)\n",
    "\n",
    "        # Create variable to store attention energies\n",
    "        attn_energies = Variable(torch.zeros(seq_len)) # B x 1 x S\n",
    "        if USE_CUDA: attn_energies = attn_energies.cuda()\n",
    "\n",
    "        # Calculate energies for each encoder output\n",
    "        for i in range(seq_len):\n",
    "            attn_energies[i] = self.score(hidden, encoder_outputs[i])\n",
    "\n",
    "        # Normalize energies to weights in range 0 to 1, resize to 1 x 1 x seq_len\n",
    "        return F.softmax(attn_energies).unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    def score(self, hidden, encoder_output):\n",
    "        \n",
    "        if self.method == 'dot':\n",
    "            energy = hidden.dot(encoder_output)\n",
    "            return energy\n",
    "        \n",
    "        elif self.method == 'general':\n",
    "            energy = self.attn(encoder_output)\n",
    "            energy = hidden.dot(energy)\n",
    "            return energy\n",
    "        \n",
    "        elif self.method == 'concat':\n",
    "            energy = self.attn(torch.cat((hidden, encoder_output), 1))\n",
    "            energy = self.other.dot(energy)\n",
    "            return energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, hidden_size, output_size, n_layers=1, dropout_p=0.1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        \n",
    "        # Keep parameters for reference\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        \n",
    "        # Define layers\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size * 2, hidden_size, n_layers, dropout=dropout_p)\n",
    "        self.out = nn.Linear(hidden_size * 2, output_size)\n",
    "        \n",
    "        # Choose attention model\n",
    "        if attn_model != 'none':\n",
    "            self.attn = Attn(attn_model, hidden_size)\n",
    "    \n",
    "    def forward(self, word_input, last_context, last_hidden, encoder_outputs):\n",
    "        # Note: we run this one step at a time\n",
    "        \n",
    "        # Get the embedding of the current input word (last output word)\n",
    "        word_embedded = self.embedding(word_input).view(1, 1, -1) # S=1 x B x N\n",
    "        \n",
    "        # Combine embedded input word and last context, run through RNN\n",
    "        rnn_input = torch.cat((word_embedded, last_context.unsqueeze(0)), 2)\n",
    "        rnn_output, hidden = self.gru(rnn_input, last_hidden)\n",
    "\n",
    "        # Calculate attention from current RNN state and all encoder outputs; apply to encoder outputs\n",
    "        attn_weights = self.attn(rnn_output.squeeze(0), encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1)) # B x 1 x N\n",
    "        \n",
    "        # Final output layer (next word prediction) using the RNN hidden state and context vector\n",
    "        rnn_output = rnn_output.squeeze(0) # S=1 x B x N -> B x N\n",
    "        context = context.squeeze(1)       # B x S=1 x N -> B x N\n",
    "        output = F.log_softmax(self.out(torch.cat((rnn_output, context), 1)))\n",
    "        \n",
    "        # Return final output, hidden state, and attention weights (for visualization)\n",
    "        return output, context, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "clip = 5.0\n",
    "\n",
    "def train(articleCaptionLang, input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    # Zero gradients of both optimizers\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss = 0 # Added onto for each word\n",
    "    \n",
    "     # Get size of input and target sentences\n",
    "    input_length = input_variable.size()[0]\n",
    "    target_length = target_variable.size()[0]\n",
    "    \n",
    "    # Run words through encoder\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "    \n",
    "    decoder_input = Variable(torch.LongTensor([[articleCaptionLang.word2index['[SOS]']]]))\n",
    "    decoder_context = Variable(torch.zeros(1, decoder.hidden_size))\n",
    "    decoder_hidden = encoder_hidden\n",
    "    if USE_CUDA:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "        decoder_context = decoder_context.cuda()\n",
    "        \n",
    "    use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
    "    if use_teacher_forcing:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_context, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output[0], target_variable[di])\n",
    "            decoder_input = target_variable[di]\n",
    "    else:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_context, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output[0], target_variable[di])\n",
    "            \n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            ni = topi[0][0]\n",
    "            \n",
    "            decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "            if USE_CUDA: decoder_input = decoder_input.cuda()\n",
    "                \n",
    "            if ni==articleCaptionLang.word2index['[EOS]']:\n",
    "                break\n",
    "                \n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm(encoder.parameters(), clip)\n",
    "    torch.nn.utils.clip_grad_norm(decoder.parameters(), clip)\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.data[0]/target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def as_minutes(s):\n",
    "    m = math.floor(s/60)\n",
    "    s -= m*60\n",
    "    return \"%dm %ds\"%(m, s)\n",
    "\n",
    "def time_since(since, percent):\n",
    "    now = time.time()\n",
    "    s = now-since\n",
    "    es = s/percent\n",
    "    rs = es - s\n",
    "    \n",
    "    return \"%s (-%s)\"%(as_minutes(s), as_minutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "attn_model = \"general\"\n",
    "hidden_size = 500\n",
    "n_layers = 2\n",
    "dropout_p = 0.05\n",
    "\n",
    "encoder = EncoderRNN(articleCaptionLang.n_words, hidden_size, n_layers)\n",
    "decoder = AttnDecoderRNN(attn_model, hidden_size, articleCaptionLang.n_words, n_layers, dropout_p=dropout_p)\n",
    "\n",
    "if USE_CUDA:\n",
    "    encoder.cuda()\n",
    "    decoder.cuda()\n",
    "    \n",
    "learning_rate = 0.0001\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 50000\n",
    "plot_every = 800\n",
    "print_every = 400\n",
    "\n",
    "start = time.time()\n",
    "plot_losses = []\n",
    "print_loss_total = 0\n",
    "plot_loss_total = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11m 58s (-1483m 54s) (400 0 %) 6.5126\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs+1):\n",
    "    training_pair = variables_from_pair(articleCaptionLang, random.choice(pairs))\n",
    "    input_variable = training_pair[0]\n",
    "    target_variable = training_pair[1]\n",
    "    \n",
    "    loss = train(articleCaptionLang, input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "    \n",
    "    print_loss_total += loss\n",
    "    plot_loss_total += loss\n",
    "    \n",
    "    if epoch == 0:continue\n",
    "        \n",
    "    if epoch%print_every == 0:\n",
    "        print_loss_avg = print_loss_total /print_every\n",
    "        print_loss_total = 0\n",
    "        print_summary = \"%s (%d %d %%) %.4f\"%(time_since(start, epoch/n_epochs), epoch, epoch/n_epochs*100, print_loss_avg)\n",
    "        print(print_summary)\n",
    "        \n",
    "    if epoch%plot_every == 0:\n",
    "        plot_loss_avg = plot_loss_total / plot_every\n",
    "        plot_losses.append(plot_loss_avg)\n",
    "        plot_loss_total = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff78e953e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgsAAAFkCAYAAACuFXjcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGRYkSVImw4IkScpkWJAkSZn+P5C5EJ8Nn0z4AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff6f63b3518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "def show_plot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "    \n",
    "show_plot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(sentence, max_length=MAX_LENGTH):\n",
    "    input_variable = variable_from_sentence(articleCaptionLang, sentence)\n",
    "    input_length = input_variable.size()[0]\n",
    "    \n",
    "    # Run through encoder\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "\n",
    "    # Create starting vectors for decoder\n",
    "    decoder_input = Variable(torch.LongTensor([[articleCaptionLang.word2index['[SOS]']]])) # SOS\n",
    "    decoder_context = Variable(torch.zeros(1, decoder.hidden_size))\n",
    "    if USE_CUDA:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "        decoder_context = decoder_context.cuda()\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    decoded_words = []\n",
    "    decoder_attentions = torch.zeros(max_length, max_length)\n",
    "    \n",
    "    # Run through decoder\n",
    "    for di in range(max_length):\n",
    "        decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_context, decoder_hidden, encoder_outputs)\n",
    "#         decoder_attentions[di,:decoder_attention.size(2)] += decoder_attention.squeeze(0).squeeze(0).cpu().data\n",
    "\n",
    "        # Choose top word from output\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi[0][0]\n",
    "        if ni == articleCaptionLang.word2index['[EOS]']:\n",
    "            decoded_words.append('<EOS>')\n",
    "            break\n",
    "        else:\n",
    "            decoded_words.append(articleCaptionLang.index2word[ni])\n",
    "            \n",
    "        # Next input is chosen word\n",
    "        decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "        if USE_CUDA: decoder_input = decoder_input.cuda()\n",
    "    \n",
    "    return decoded_words, decoder_attentions[:di+1, :len(encoder_outputs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_randomly():\n",
    "    pair = random.choice(pairs)\n",
    "    \n",
    "    output_words, decoder_attn = evaluate(pair[0])\n",
    "    output_sentence = ' '.join(output_words)\n",
    "    \n",
    "    print('>', pair[0])\n",
    "    print('=', pair[1])\n",
    "    print('<', output_sentence)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Jean Claude Baby Doc Duvalier the deposed playboy President of Haiti died earlier this month in the Haitian capital of Port au Prince apparently of natural causes. He was . His father Fran ois Papa Doc Duvalier had appointed Jean Claude President for life in at the age of only . Duvalier p re entertained more than an anthropological interest in Afro Caribbean ritual. His wardrobe of black suits and black homburgs lent him the aspect Haitians say of the Vodou divinity Baron Samedi who haunts the graveyards in a top hat and tails like a ghoulish Groucho Marx. Baby Doc was no less unusual a dictator. Having misruled Haiti for years he and his beautiful Haitian wife Mich le Bennett were overthrown in the popular uprising of February and fled first to France then to the United States. Their hurried departure overseas became the subject of Christopher Hope s entertaining novel My Chocolate Redemeer where a deposed Caribbean dictator commandeers two floors of a hotel in the South of France and expels any guests who stand in his large and dangerous way. In out of the blue Baby Doc returned to Haiti reportedly to help put his homeland back on its feet after the terrible earthquake of the previous year. In Port au Prince he was allowed to go about his business unmolested but many Haitians suspected that he was back solely to reclaim funds ransacked from the state coffers. Some years ago hoping to interview Baby Doc I made my way to a hush hush address in Queens New York where Duvalierist exiles had gathered to plot his restoration to power. It was the autumn of . Dr Franz Bataille who edited the pro Duvalier newspaper Haiti Observateur puffed importantly on a cigar while his girlfriend emerged from a back room with a plate of fried pork and a bottle of Haitian Barbancourt rum for us. In lachrymose tones Dr Bataille spoke of the golden years of Haiti under Baby Doc and added tearfully Jean Claude is an angel. Haitians would jump for joy to have him back. Really? The Tontons Macoute private militia set up by his father in had caused tens of thousands to flee Haiti in terror. Moreover the palace in Port au Prince where Baby Doc lived with his wife was a picture of excess. Spring loaded security doors led to private apartments filled with giant artificial banana trees fashioned from bamboo with mirrored leaves and fronds huge gold snails in the bathroom and great fluffy footballs behind Chinese vellum screens. In the presidential chapel a small projector screen hung above the altar Baby Doc had a penchant for pornography in Technicolor. Proceeds for his wedding in derived largely from drug trafficking. Held in Port au Prince cathedral it cost an estimated US m m couture gowns and hairdressers imported from Paris along with a firework display ensured that the wedding made the Guinness World Records book for immoderation. Unfortunately festivities were marred somewhat by a torrential downpour that caused the city sewers to overflow. With a sigh Dr Bataille got up and dialled a long number. I watched as he bowed to the phone. Bonjour Monsieur le Pr sident comment allez vous? I could feel the tension in the room. Yes we have Mr Thomson here. I ll pass him over to you. Taking the phone I repeated Dr Bataille s fawning words to Baby Doc. Good morning Mr President. How are you? Baby Doc spoke so slowly in reply that I thought that he must be drugged or ill. In fact he was ailing from heart problems aggravated by diabetes. After more suitably ingratiating remarks from me the ex dictator agreed to be interviewed for a newspaper article but asked me to contact him in his Paris exile once I had returned home. Back in Stoke Newington I telephoned Baby Doc on November Guy Fawkes Day which proved to be a mistake. The instant Monsieur le Pr sident picked up the phone a neighbour let off fireworks beneath my window. There was a long silence as I struggled to speak above a background detonation of rockets. No reply was forthcoming. In increasingly strained French I tried to explain to Baby Doc the significance of the Gunpowder Plot. But Baby Doc wasn t listening any more. The phone went dead and I never got the interview. It is estimated that Baby Doc and Bennett embezzled between them some m to m. When proceedings to recover the money commenced in July the defendants claimed plausibly enough that it had been a tradition in Haiti for almost two centuries for a new government to take legal action against the previous regime. Bennett tired of living with the man Haitians nicknamed Baskethead divorced Baby Doc in . The National Palace where the dictator and his wife had lavished minions with champagne and honorific titles was destroyed in the earthquake. Not even the giant artificial banana trees remain. The new post earthquake edition of Bonjour Blanc A Journey through Haiti by Ian Thomson Vintage is published this week\n",
      "= How Guy Fawkes Day ruined my Baby Doc Duvalier interview\n",
      "< years the s to to to to to <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_randomly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
